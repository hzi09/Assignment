# 1. ì§€ë„í•™ìŠµ(Supervised Learning)


## ğŸ—‚ï¸Directory Structure
```bash
ğŸ“¦1. Supervised-Learning
 â”£ ğŸ“œhousingdata.csv
 â”£ ğŸ“œREADME.md
 â”£ ğŸ“œSupervised_Learning_v1.ipynb
 â”£ ğŸ“œSupervised_Learning_v2.ipynb
 â”£ ğŸ“œSupervised_Learning_v3.ipynb
 â”— ğŸ“œSupervised_Learning_v4.ipynb
``` 
- `housingdata.csv` : ë°ì´í„° CSV íŒŒì¼
- `README.md` : ë””ë ‰í† ë¦¬ êµ¬ì¡°, ê³¼ì œ ë‚´ìš©, ë¬¸ì œ í’€ì´
- `Supervised_Learning_v1.ipynb` : ë©”ì¸ ê³¼ì œ
- `Supervised_Learning_v1.ipynb` : ë„ì „ ê³¼ì œ - ëª¨ë¸ ì•™ìƒë¸”
- `Supervised_Learning_v1.ipynb` : ë„ì „ ê³¼ì œ - í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹
- `Supervised_Learning_v1.ipynb` : ë„ì „ ê³¼ì œ - ì‹œê°„ì  ìš”ì†Œ ì¶”ê°€


<br>

## ê³¼ì œ ë‚´ìš©
### ğŸ’¡ì£¼ì œ
ì£¼íƒ ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•

### ğŸ¯ëª©í‘œ
ì£¼ì–´ì§„ ì£¼íƒ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì£¼íƒ ê°€ê²©ì„ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

### ğŸ“–í•™ìŠµë‚´ìš©
- ì§€ë„ í•™ìŠµì˜ ê¸°ë³¸ ê°œë…ê³¼ íšŒê·€ ë¶„ì„ì„ ì´í•´í•˜ê³ , ì‹¤ì œ ë°ì´í„°ì— ì ìš©í•˜ëŠ” ëŠ¥ë ¥
- ë°ì´í„° ì „ì²˜ë¦¬ ë° íƒìƒ‰: ë°ì´í„°ì˜ í’ˆì§ˆì„ ë†’ì´ëŠ” ë°©ë²•ê³¼ íŠ¹ì§• ì„ íƒì˜ ì¤‘ìš”ì„±
- ì—¬ëŸ¬ íšŒê·€ ëª¨ë¸ì˜ ì´í•´: ë‹¤ì–‘í•œ íšŒê·€ ê¸°ë²•ì˜ ì›ë¦¬ì™€ ì ìš© ë°©ë²•
- ëª¨ë¸ ì„±ëŠ¥ í‰ê°€: ì„±ëŠ¥ ì§€í‘œì˜ ì´í•´ ë° ë¹„êµ ë¶„ì„ì„ í†µí•´ ìµœì ì˜ ëª¨ë¸ ì„ íƒ


## ê³¼ì œ ê°€ì´ë“œ
- **ë°ì´í„°ì…‹ íƒìƒ‰ ë° ì „ì²˜ë¦¬**:
    - ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    - ì´ìƒì¹˜ íƒì§€ ë° ì œê±°
    - íŠ¹ì§• ì„ íƒ
- **ì—¬ëŸ¬ íšŒê·€ ëª¨ë¸ ë¹„êµ**:
    - ì„ í˜• íšŒê·€
    - ì˜ì‚¬ê²°ì •ë‚˜ë¬´
    - ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë“±
- **ëª¨ë¸ ì„±ëŠ¥ í‰ê°€**:
    - ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.
        - **Mean Absolute Error (MAE)**: ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì ˆëŒ€ ì˜¤ì°¨ì˜ í‰ê· .
        - **Mean Squared Error (MSE)**: ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì œê³± ì˜¤ì°¨ì˜ í‰ê· .
        - **RÂ² Score**: ëª¨ë¸ì´ ë°ì´í„°ì˜ ë³€ë™ì„±ì„ ì–¼ë§ˆë‚˜ ì„¤ëª…í•˜ëŠ”ì§€ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ.
- **ê²°ê³¼ ë¶„ì„**:
    - ê° ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ê³  ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•˜ì—¬ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
        - **ì‹œê°í™”**: ì„±ëŠ¥ ì§€í‘œë¥¼ ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ì‰½ê²Œ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. matplotlib ë˜ëŠ” seabornì„ ì‚¬ìš©í•˜ì—¬ ë§‰ëŒ€ ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.


<br>

## ğŸ”¥ì¶”ê°€ ë„ì „ ê³¼ì œ
- **ëª¨ë¸ ì•™ìƒë¸”** â­â­â­â­â­
    - ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì•™ìƒë¸” ê¸°ë²•(ì˜ˆ: ë°°ê¹…, ë¶€ìŠ¤íŒ…)ì„ ì ìš©í•©ë‹ˆë‹¤.
    - ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ í‰ê· ë‚´ê±°ë‚˜ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ìƒì„±í•©ë‹ˆë‹¤.
- **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹** â­â­â­â­
    - Grid Search ë˜ëŠ” Random Search ê¸°ë²•ì„ ì´ìš©í•´ ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.
- **ì‹œê°„ì  ìš”ì†Œ ì¶”ê°€** â­â­â­
    - ì£¼íƒ ë°ì´í„°ì…‹ì— ì‹œê°„ì  ìš”ì†Œ(ì˜ˆ: ê³„ì ˆì  ë³€í™”, ê²½ì œ ì§€í‘œ ë“±)ë¥¼ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ë ¥ì„ ë†’ì…ë‹ˆë‹¤.


<br>

## âœï¸ë¬¸ì œ í’€ì´
### ë°ì´í„°ì…‹ íƒìƒ‰ ë° ì „ì²˜ë¦¬

#### 1. ë°ì´í„° í™•ì¸

- ë¨¸ì‹ ëŸ¬ë‹ì„ ì§„í–‰í•˜ê¸° ì „ì— ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  í™•ì¸
  - CSV íŒŒì¼ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° : `read_csv()`
  - ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°(5ê°œì˜ í–‰ë§Œ í™•ì¸) : `head()`
  - ë°ì´í„°ì…‹ì˜ ê¸°ë³¸ ì •ë³´ í™•ì¸ : `info()`
    - ë°ì´í„° íƒ€ì…, ì¸ë±ìŠ¤ ê°œìˆ˜, ì»¬ëŸ¼ëª… ë“±ì„ í™•ì¸ ê°€ëŠ¥
  - ë°ì´í„° ì •ë³´ íƒìƒ‰ : `describe()` 
    - ê° ì»¬ëŸ¼ë³„ ë°ì´í„°ì˜ ê°œìˆ˜, í‰ê· ê°’, ìµœëŒ€ê°’ ë“±ì„ í™•ì¸ ê°€ëŠ¥
    ```python
    import pandas as pd

    # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    housing = pd.read_csv('housingdata.csv')

    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    housing.head()

    # ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´ í™•ì¸
    housing.info() 

    # ë°ì´í„° ì •ë³´ íƒìƒ‰
    housing.describe() 
    ```
- ìœ„ ë°ì´í„°ë¥¼ ì‹œê°í™”í•´ì„œ í™•ì¸
  - íˆìŠ¤í† ê·¸ë¨(Histogram) : `hist()`
    - `bins` : ê°€ë¡œì¶• êµ¬ê°„ì˜ ê°œìˆ˜ëŠ” ì¢€ ì—¬ìœ ìˆê²Œ 50ìœ¼ë¡œ ê²°ì •
    - `figsize` : ê°€ë¡œê¸¸ì´ì™€ ì„¸ë¡œê¸¸ì´ëŠ” ì‘ì„±í•˜ë©´ì„œ ì—¬ìœ ìˆê²Œ ì§€ì •í•´ì£¼ì—ˆë‹¤.(x=20, y=15)
    ```python
    import matplotlib.pyplot as plt

    housing.hist(bins=50, figsize=(20,15))
    plt.show()
    ```

#### 2. ë°ì´í„° ì „ì²˜ë¦¬

- ê²°ì¸¡ì¹˜ ì²˜ë¦¬
   - ê²°ì¸¡ì¹˜ì˜ ê°œìˆ˜ í™•ì¸ : `isna().sum()`
     - CRIM, ZN, INDUS, CHAS, AGE, LSTAT ì»¬ëŸ¼ì— 20ê°œì”©ì˜ ê²°ì¸¡ì¹˜ê°€ í™•ì¸ë¨
        ```python
        # ê²°ì¸¡ì¹˜ ê°œìˆ˜ í™•ì¸
        housing.isna().sum()
        ```
   - ê²°ì¸¡ì¹˜ ë¹„ìœ¨ í™•ì¸ 
     - ê²°ì¸¡ì¹˜ì˜ ê°œìˆ˜ë¥¼ ì „ì²´ ë°ì´í„°ì˜ ê¸¸ì´ë¡œ ë‚˜ëˆˆí›„ *100ì„ í•˜ë©´ ê²°ì¸¡ì¹˜ì˜ ë¹„ìœ¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ
        ```python
        # ê²°ì¸¡ì¹˜ ë¹„ìœ¨ í™•ì¸
        missing_percentage= (housing.isnull().sum() / len(housing)) * 100
        ```
    - ê²°ê³¼
      - ê·¸ë ‡ê²Œ ë†’ì€ ìˆ˜ì¹˜ëŠ” ì•„ë‹ˆë¼ì„œ ê·¸ëƒ¥ ì‚­ì œë¥¼ í• ê¹Œ í•˜ë‹¤ê°€, ê²°ì¸¡ì¹˜ì²˜ë¦¬ë¥¼ ì§„í–‰
        ```
        CRIM       3.952569
        ZN         3.952569
        INDUS      3.952569
        CHAS       3.952569
        NOX        0.000000
        RM         0.000000
        AGE        3.952569
        DIS        0.000000
        RAD        0.000000
        TAX        0.000000
        PTRATIO    0.000000
        B          0.000000
        LSTAT      3.952569
        MEDV       0.000000
        dtype: float64
        ```
- ê²°ì¸¡ì¹˜ ì²˜ë¦¬
  - ìˆ˜ì¹˜í˜•ë°ì´í„°ëŠ” `mean`ê³¼ `median` ì¤‘ ê³ ë¯¼í•˜ì˜€ì§€ë§Œ, ë°ì´í„° ë²”ìœ„ê°€ í° ë°ì´í„°ë„ ìˆì–´ì„œ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
  - ë²”ì£¼í˜• ë°ì´í„°ëŠ” `most_frequent`(ìµœë¹ˆê°’)ë¡œ ëŒ€ì²´
    ```python
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    from sklearn.impute import SimpleImputer

    # ìˆ˜ì¹˜í˜•ë°ì´í„° ê²°ì¸¡ì¹˜ > ì¤‘ì•™ê°’ ëŒ€ì²´
    imputer = SimpleImputer(strategy='median')

    for col in ['CRIM', 'ZN', 'INDUS', 'AGE', 'LSTAT'] :
        housing[col] = imputer.fit_transform(housing[[col]])

    # ë²”ì£¼í˜• ë°ì´í„° ê²°ì¸¡ì¹˜ > ìµœë¹ˆê°’ ëŒ€ì²´
    imputer2 = SimpleImputer(strategy= 'most_frequent')

    housing['CHAS'] = imputer2.fit_transform(housing[['CHAS']])
    ```

- ì´ìƒì¹˜ ì²˜ë¦¬
  - ì´ìƒì¹˜ ì²˜ë¦¬ ì „ì— ê¸°ì¡´ì˜ ë°ì´í„°ì™€ ë¹„êµë¥¼ í•˜ê³  ì‹¶ì–´ì„œ `housing_processed` ë³€ìˆ˜ë¥¼ ë§Œë“¤ì–´ì„œ ë°ì´í„°ë¥¼ ë³µì‚¬(`copy()`)í•´ ë„£ì–´ì¤Œ
  - ì´ìƒì¹˜ëŠ” IQR ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•˜ì˜€ìœ¼ë©° ê²½ê³„ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” ë°©ë²•ì„ ì±„íƒ 
    ```python
    import numpy as np

    numeric_cols = housing.select_dtypes(include=[np.number]).columns # ìˆ˜ì¹˜í˜• ë°ì´í„°ë¥¼ ê°€ì§„ ì—´ë“¤ì˜ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
    housing_processed = housing.copy()


    for col in numeric_cols:
        Q1 = housing[col].quantile(0.25)
        Q3 = housing[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = ((housing[col] < lower_bound) | (housing[col] > upper_bound))
        outlier_count = outliers.sum()

        if outlier_count > 0:
            # ì´ìƒì¹˜ë¥¼ ê²½ê³„ê°’ìœ¼ë¡œ ëŒ€ì²´ (Winsorization)
            housing_processed[col] = housing_processed[col].clip(lower=lower_bound, upper=upper_bound)
    ```

  - ì´ìƒì¹˜ ì²˜ë¦¬ ì „ í›„ë¥¼ ì‹œê°í™”í•˜ì—¬ í‘œí˜„
    ```python
    # ì´ìƒì¹˜ ì²˜ë¦¬ ì „ ë°ì´í„° ì‹œê°í™”
    housing.boxplot()
    plt.xticks(rotation=90)
    plt.title('Before Boxplot')
    plt.tight_layout()
    plt.show()

    # ì´ìƒì¹˜ ì²˜ë¦¬ í›„ ë°ì´í„° ì‹œê°í™”
    housing_processed.boxplot()
    plt.xticks(rotation=90)
    plt.title('After Boxplot')
    plt.tight_layout()
    plt.show()
    ```
    - ê²°ê³¼
        ![image](https://github.com/user-attachments/assets/8d30ff39-9477-4947-9d3f-f26e1fdfae7e)

- ìƒê´€ê´€ê³„
  - í•™ìŠµì„ ì‹œì‘í•˜ê¸° ì „ì— íƒ€ê²Ÿ ë°ì´í„°ì™€ ìƒê´€ê´€ê³„ë¥¼ í™•ì¸ : `corr()`
  - ìƒê´€ê´€ê³„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ë°ì´í„° í™•ì¸í•˜ê¸°
    - ë°ì´í„°ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ ë°”ê¾¸ì–´ì¤€ í›„ : `avs()`
    - ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ : `sort_values(ascending=False)`
        ```python
        # ë°ì´í„° ìƒê´€ê´€ê³„ í™•ì¸
        corr_matrix = housing_processed.corr()
        corr_matrix['MEDV'].abs().sort_values(ascending=False)
        ```
    - ê²°ê³¼
        ```
        MEDV       1.000000
        LSTAT      0.782941
        RM         0.697645
        INDUS      0.553140
        TAX        0.543545
        PTRATIO    0.523993
        CRIM       0.522140
        NOX        0.506505
        AGE        0.454330
        RAD        0.452679
        DIS        0.333079
        B          0.321250
        ZN              NaN
        CHAS            NaN
        Name: MEDV, dtype: float64
        ```
  - ì‹œê°í™” ë¶„ì„ : ì„ í˜• íšŒê·€ì„  í™•ì¸
    - MEDVì™€ ì–¼ë§ˆë‚˜ ì„ í˜•ì ì¸ ê´€ê³„ê°€ ìˆëŠ”ì§€ í™•ì¸
      - `ZN`ê³¼ `CHAS`ëŠ” ì„ í˜•ê´€ê³„ê°€ ì—†ë‹¤ê³  íŒë‹¨ë˜ì–´ì„œ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
      - ì˜¤ë¥¸ìª½ ì•„ë˜ë¡œ ë‚´ë ¤ê°€ëŠ” ì„  : ìŒì˜ ìƒê´€ê´€ê³„
      - ì˜¤ë¥¸ìª½ ìœ„ë¡œ ì˜¬ë¼ê°€ëŠ” ì„  : ì–‘ì˜ ìƒê´€ê´€ê³„
        ```python
        import seaborn as sns

        # ì‹œê°í™” ë¶„ì„
        plot_cols = ['MEDV', 'LSTAT', 'RM', 'INDUS', 'TAX', 'PTRATIO', 'CRIM', 'NOX', 'AGE', 'RAD', 'DIS', 'B']
        plot_housing = housing.loc[:, plot_cols]

        # ì„ í˜• íšŒê·€ì„  í‘œì‹œ
        fig, axs = plt.subplots(figsize=(16, 10))
        for i, feature in enumerate(plot_cols[1:]) :
            ax1 = plt.subplot(3,4,i+1)
            sns.regplot(x=feature, y=plot_cols[0], data=plot_housing, ax=ax1)
        ```
        ![image](https://github.com/user-attachments/assets/8063c063-441f-4cc4-84f6-9ef0813b44d9)

  - ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ í™•ì¸
    - ìƒê´€ê´€ê³„ë¥¼ ì‹œê°ì ìœ¼ë¡œ ëª…í™•í•˜ê²Œ í™•ì¸í•  ìˆ˜ ìˆëŠ” íˆíŠ¸ë§µì„ ì‚¬ìš©
    - `DIS`ì™€ `B`ê°€ ìƒê´€ê´€ê³„ê°€ ë‚®ì€ ê±¸ í™•ì¸ ê°€ëŠ¥
        ```python
        # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
        plt.figure(figsize=(8, 6))
        sns.heatmap(plot_housing.corr(), annot=True, cmap='coolwarm', fmt='.2f')
        plt.title('Correlation Heatmap')
        plt.show()
        ```
        ![image](https://github.com/user-attachments/assets/f0ba40d7-80d2-45a5-9d75-1ac1bd4b9c00)


### ì—¬ëŸ¬ íšŒê·€ ëª¨ë¸ ë¹„êµ
#### 3. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
- ì‚¬ìš©í•  íŠ¹ì„±(ë…ë¦½ë³€ìˆ˜)ëŠ” 'LSTAT', 'RM', 'INDUS', 'TAX', 'PTRATIO', 'CRIM', 'NOX', 'AGE', 'RAD'
- íƒ€ê²Ÿ ë³€ìˆ˜(ë§ê·¸ëŒ€ë¡œ ì˜ˆì¸¡í•  ë°ì´í„°)ëŠ” 'MEDV'
    ```python
    # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
    X = housing_processed[['LSTAT', 'RM', 'INDUS', 'TAX', 'PTRATIO', 'CRIM', 'NOX', 'AGE', 'RAD']]  # ë…ë¦½ ë³€ìˆ˜
    y = housing_processed['MEDV']  # íƒ€ê²Ÿ ë³€ìˆ˜
    ```

#### 4. ëª¨ë¸ ì„ íƒ ë° í›ˆë ¨
- ì„ í˜•íšŒê·€, ë‹¤í•­íšŒê·€, ì˜ì‚¬ê²°ì •ë‚˜ë¬´, ëœë¤í¬ë ˆìŠ¤íŠ¸, ë¦¬ì§€ íšŒê·€, ë¼ì˜ íšŒê·€ì— ëŒ€í•´ì„œ í›ˆë ¨ì„ ì§„í–‰


### ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ê²°ê³¼ë¶„ì„
#### 5. ëª¨ë¸ í‰ê°€
- ëª¨ë¸ ë³„ë¡œ ì„±ëŠ¥(MAE, MSE, R2)ë¥¼ êµ¬í•˜ì—¬ ê²°ê³¼ë¥¼ ë¹„êµ
    ```python
    # ëª¨ë¸ë³„ ì„±ëŠ¥ ì €ì¥
    results = pd.DataFrame({
        'Model': ['Linear Regression', 'Polynomial Regression', 'Decision Tree', 'Random Forest', 'Ridge Regression', 'Lasso Regression'],
        'MAE': [lr_mae, pr_mae, tree_mae, rf_mae, ri_mae, ls_mae],
        'MSE': [lr_mse, pr_mse, tree_mse, rf_mse, ri_mse, ls_mse],
        'R2': [lr_r2, pr_r2, tree_r2, rf_r2, ri_r2, ls_r2]
    })

    # ì„œë¸Œí”Œë¡¯ ìƒì„±
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))  

    colors = ['skyblue', 'green', 'purple']

    # ê·¸ë˜í”„ë¥¼ ê°ê° ìƒì„±
    for ax, metric, color in zip(axes, ['MAE', 'MSE', 'R2'], colors):
        ax.bar(results['Model'], results[metric], color=color, width=0.5)
        ax.set_title(f'{metric} Comparison')
        ax.set_ylabel(f'{metric} Value')
        ax.set_xlabel('Model')
        ax.grid(axis='y', linestyle='--', alpha=0.7)
        # xì¶• ê¸°ìš¸ê¸°
        ax.set_xticks(np.arange(len(results['Model'])))
        ax.set_xticklabels(results['Model'], rotation=70)

    # ë ˆì´ì•„ì›ƒ ì¡°ì • ë° ì¶œë ¥
    plt.tight_layout()
    plt.show()    
    ```
    ![image](https://github.com/user-attachments/assets/70521ddb-ffc9-46f1-af3d-983b2c4827fb)


- ë¼ì˜íšŒê·€ê°€ R2ê°’ì´ ê°€ì¥ ë†’ì€ ê²ƒìœ¼ë¡œ í™•ì¸ë˜ì—ˆë‹¤.
  
    |                           | ì„ í˜•íšŒê·€ | ë‹¤í•­íšŒê·€ | ì˜ì‚¬ê²°ì •ë‚˜ë¬´ | ëœë¤í¬ë ˆìŠ¤íŠ¸ | ë¦¬ì§€íšŒê·€ | ë¼ì˜íšŒê·€ |
    |--------------------------:|---------:|---------:|-------------:|-------------:|---------:|---------:|
    | Mean Absolute Error (MAE) |   2.4453 |   2.1346 |       2.6892 |       1.9230 |   2.4510 |   1.8711 |
    |  Mean Squared Error (MSE) |  13.0004 |   8.7704 |      13.7651 |       6.3052 |  13.0244 |   5.9593 |
    |                  RÂ² Score |   0.7342 |   0.8207 |       0.7186 |       0.8711 |   0.7337 |   0.8781 |


## âœï¸ ë„ì „ê³¼ì œ ë¬¸ì œ í’€ì´
### ëª¨ë¸ ì•™ìƒë¸”
- ë°°ê¹… ëª¨ë¸ê³¼ ë¶€ìŠ¤íŒ… ëª¨ë¸ì„ ì¶”ê°€í•œ í›„(ì—¬ê¸°ì„œëŠ” ì½”ë“œ ìƒëµ), ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”ì„ ì§„í–‰
  - ê°€ì¤‘ ì•™ìƒë¸” : ì„±ëŠ¥ì´ ë†’ì€ ëª¨ë¸ 5ê°œë¥¼ ì„ íƒ
    ```python
    # r2ì ìˆ˜ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    model_pred = [y_lr_pred, y_pr_pred, y_tree_pred, y_rf_pred, y_ri_pred, y_ls_pred, y_bagging_pred, y_boosting_pred]
    model_r2_score = [lr_r2, pr_r2, tree_r2, rf_r2, ri_r2, ls_r2, bagging_r2, boosting_r2]

    print(model_r2_score)

    # ì„±ëŠ¥ì´ ë†’ì€ 5ê°œ ëª¨ë¸ ì„ íƒ
    top_models_idx = np.argsort(model_r2_score)[-5:]  # ìƒìœ„ 5ê°œ ëª¨ë¸ ì¸ë±ìŠ¤
    top_pred = [model_pred[i] for i in top_models_idx]
    top_scores = [model_r2_score[i] for i in top_models_idx]

    # ê°€ì¤‘ì¹˜ ê³„ì‚° (ì„±ëŠ¥ ê¸°ë°˜)
    weights = np.array(top_scores) / sum(top_scores)

    # ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”
    final_pred = sum(w * pred for w, pred in zip(weights, top_pred))
    
    # ì„±ëŠ¥í‰ê°€
    final_mae = mean_absolute_error(y_test, final_pred)
    final_mse = mean_squared_error(y_test, final_pred)
    final_r2 = r2_score(y_test, final_pred)

    print(f'Mean Absolute Error (MAE) : {final_mae}')
    print(f'Mean Squared Error (MSE) : {final_mse}')
    print(f'RÂ² Score : {final_r2}')
    ```
  - ê²°ê³¼
    ```
    Mean Absolute Error (MAE) : 1.920664657312724
    Mean Squared Error (MSE) : 6.670866427584049
    RÂ² Score : 0.8636510323472387
    ```

- ë¼ì˜ ëª¨ë¸ë³´ë‹¤ ê²°ê³¼ê°€ ë‚®ê²Œ ë‚˜ì˜´
  - ì•„ë§ˆ ê°€ì¤‘ì¹˜ ë¶€ì—¬ë¥¼ í•˜ì§€ ì•Šì•„ì„œê°€ ì•„ë‹ê¹Œ ì‹¶ì€ë° ì´ ë¶€ë¶„ì€ ì¶”ê°€ í•™ìŠµ í›„ì— ë‹¤ì‹œ ìˆ˜ì •ì´ í•„ìš”í•  ê²ƒ ê°™ìŒ


    |                           | ì„ í˜•íšŒê·€ | ë‹¤í•­íšŒê·€ | ì˜ì‚¬ê²°ì •ë‚˜ë¬´ | ëœë¤í¬ë ˆìŠ¤íŠ¸ | ë¦¬ì§€íšŒê·€ | ë¼ì˜íšŒê·€ | ë°°ê¹…ëª¨ë¸ | ë¶€ìŠ¤íŒ…ëª¨ë¸ | ê°€ì¤‘í‰ê· ì•™ìƒë¸” |
    |--------------------------:|---------:|---------:|-------------:|-------------:|---------:|---------:|---------:|-----------:|-----------------:|
    | Mean Absolute Error (MAE) |   2.4453 |   2.1346 |       2.6892 |       1.9230 |   2.4510 |   1.8711 |   1.9380 |     1.9168 |           1.9206 |
    |  Mean Squared Error (MSE) |  13.0004 |   8.7704 |      13.7651 |       6.3052 |  13.0244 |   5.9593 |   6.3964 |     6.4878 |           6.6708 |
    |                  RÂ² Score |   0.7342 |   0.8207 |       0.7186 |       0.8711 |   0.7337 |   0.8781 |   0.8692 |     0.8673 |           0.8636 |


### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

- ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì§„í–‰
  - ê·¸ë¦¬ë“œ ì„œì¹˜
    ```python
    from sklearn.model_selection import GridSearchCV

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„¤ì •
    param_grid = {
        'n_estimators': [300], 
        'max_depth': [3, 5, 7, 10], 
        'min_samples_split': [6 ,8, 12, 18],
        'min_samples_leaf': [6, 8, 16, 20],
    }


    rf_clf = RandomForestRegressor(random_state=0, n_jobs=-1) # n_jobs = -1 : ëª¨ë“  CPU ì½”ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ
    grid_cv = GridSearchCV(rf_clf, param_grid=param_grid, cv=5, n_jobs=-1)
    grid_cv.fit(X_train, y_train)

    # ë°ì´í„° ë¶„í•  (í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡
    best_rf_model = RandomForestRegressor(n_estimators=300, max_depth=10, min_samples_leaf=6, min_samples_split=6)
    best_rf_model.fit(X_train_scaled, y_train)
    y_rf_pred = best_rf_model.predict(X_test_scaled)

    # ëª¨ë¸ í‰ê°€
    rf_mae = mean_absolute_error(y_test, y_rf_pred)
    rf_mse = mean_squared_error(y_test, y_rf_pred)
    rf_r2 = r2_score(y_test, y_rf_pred)

    print(f'Mean Absolute Error (MAE): {rf_mae}')
    print(f'Mean Squared Error (MSE): {rf_mse}')
    print(f'RÂ² Score: {rf_r2}')
    ```

- ëœë¤ ì„œì¹˜
    ```python
    from sklearn.model_selection import RandomizedSearchCV

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶„í¬ ì„¤ì •
    param_dist = {
        'n_estimators': [300],
        'max_depth': [5, 7, 10, 15], 
        'min_samples_split': [4 ,6 ,8, 12],
        'min_samples_leaf': [6, 8, 16, 20],
        'bootstrap': [True, False]
    }


    rf_clf = RandomForestRegressor(random_state=0, n_jobs=-1) # n_jobs = -1 : ëª¨ë“  CPU ì½”ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ
    random_cv = RandomizedSearchCV(rf_clf, param_distributions=param_dist, cv=5, n_jobs=-1)
    random_cv.fit(X_train, y_train)

    # ë°ì´í„° ë¶„í•  (í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡
    best_rf_model = RandomForestRegressor(n_estimators=300, max_depth=7, min_samples_leaf=6, min_samples_split=4, bootstrap=True)
    best_rf_model.fit(X_train_scaled, y_train)
    y_rf_pred = best_rf_model.predict(X_test_scaled)

    # ëª¨ë¸ í‰ê°€
    rf_mae = mean_absolute_error(y_test, y_rf_pred)
    rf_mse = mean_squared_error(y_test, y_rf_pred)
    rf_r2 = r2_score(y_test, y_rf_pred)

    print(f'Mean Absolute Error (MAE): {rf_mae}')
    print(f'Mean Squared Error (MSE): {rf_mse}')
    print(f'RÂ² Score: {rf_r2}')
    ```

- ê²°ê³¼
  - í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì§„í–‰í–ˆì§€ë§Œ ê·¸ë¦¬ë“œì„œì¹˜ ëœë¤ì„œì¹˜ ë‘ê°œ ëª¨ë‘ RÂ²ê°€ ë–¨ì–´ì§„ê±¸ í™•ì¸
  - ì´ ë¶€ë¶„ë„ í•™ìŠµì´ ì¡°ê¸ˆ ë” í•„ìš”
    |                           | ëœë¤í¬ë ˆìŠ¤íŠ¸ | ê·¸ë¦¬ë“œì„œì¹˜ | ëœë¤ì„œì¹˜ |
    |--------------------------:|-------------:|-----------:|---------:|
    | Mean Absolute Error (MAE) |       1.9230 |     2.0244 |   2.0240 |
    |  Mean Squared Error (MSE) |       6.3052 |     8.1637 |   8.0601 |
    |                  RÂ² Score |       0.8711 |     0.8331 |   0.8352 | 


### ì‹œê°„ì  ìš”ì†Œ ì¶”ê°€
- ë°ì´í„° ì „ì²˜ë¦¬ í›„ì— ì‹œê°„ì  ìš”ì†Œë¥¼ ì¶”ê°€
  - ì–´ë–¤ ìš”ì†Œê°€ ê°€ì¥ ì í•©í• ì§€ ìƒê°í•œ ëì— `TAX`ë¡œ ê²°ì • (TAX: 10,000ë‹¬ëŸ¬ë‹¹ ì¬ì‚°ì„¸ìœ¨)
- `TAX`ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ì •ë ¬
    ```python
    # ì´ìƒì¹˜ê°€ ì²˜ë¦¬ëœ housing ë°ì´í„°ì—ì„œ TAX ê°’ìœ¼ë¡œ ë°ì´í„° ì •ë ¬
    housing_processed = housing_processed.sort_values(by="TAX").reset_index(drop=True)
    housing_processed
    ```
- `TAX`ê°€ ì˜¬ë¼ê°ˆë•Œë§ˆë‹¤ 1ì´ ì˜¬ë¼ê°€ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰
  - ì‹¤ì œ ë…„ë„ë¥¼ ì•Œ ìˆ˜ ì—†ì–´ ë°ì´í„°ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ 1ì”© ì¦ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ í‘œí˜„í•¨
  - ë¬¼ë¡  ì„¸ê¸ˆì´ ë–¨ì–´ì§ˆë•Œë„ ìˆì–´ ì´ ë°ì´í„°ë¥¼ ì‹œê°„ë°ì´í„°ë¡œ ë°”ê¾¸ëŠ” ê²ƒì€ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ì„ íƒì¼ìˆ˜ë„ ìˆìŒ
    ```python
    # TAXê°’ì´ ì˜¬ë¼ê°ˆë•Œë§ˆë‹¤ 1ì„ ë”í•˜ì—¬ ì‹œê°„ì˜ íë¦„ì„ í‘œí˜„
    housing_processed['TIME'] = (housing_processed['TAX'] != housing_processed['TAX'].shift()).cumsum()
    housing_processed
    ```
- `MEDV`ì™€ -0.51ë¡œ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ê²ƒìœ¼ë¡œ í™•ì¸
- ê²°ê³¼ ë¹„êµ
  - ê¸°ì¡´ ê²°ê³¼
    |                           | ì„ í˜•íšŒê·€ | ë‹¤í•­íšŒê·€ | ì˜ì‚¬ê²°ì •ë‚˜ë¬´ | ëœë¤í¬ë ˆìŠ¤íŠ¸ | ë¦¬ì§€íšŒê·€ | ë¼ì˜íšŒê·€ | ë°°ê¹…ëª¨ë¸ | ë¶€ìŠ¤íŒ…ëª¨ë¸ | ê°€ì¤‘í‰ê· ì•™ìƒë¸” |
    |--------------------------:|---------:|---------:|-------------:|-------------:|---------:|---------:|---------:|-----------:|-----------------:|
    | Mean Absolute Error (MAE) |   2.4453 |   2.1346 |       2.6892 |       1.9230 |   2.4510 |   1.8711 |   1.9380 |     1.9168 |           1.9206 |
    |  Mean Squared Error (MSE) |  13.0004 |   8.7704 |      13.7651 |       6.3052 |  13.0244 |   5.9593 |   6.3964 |     6.4878 |           6.6708 |
    |                  RÂ² Score |   0.7342 |   0.8207 |       0.7186 |       0.8711 |   0.7337 |   0.8781 |   0.8692 |     0.8673 |           0.8636 |
    
  - `TAX` ì¶”ê°€ ê²°ê³¼
    |                           | ì„ í˜•íšŒê·€ | ë‹¤í•­íšŒê·€ | ì˜ì‚¬ê²°ì •ë‚˜ë¬´ | ëœë¤í¬ë ˆìŠ¤íŠ¸ | ë¦¬ì§€íšŒê·€ | ë¼ì˜íšŒê·€ | ë°°ê¹…ëª¨ë¸ | ë¶€ìŠ¤íŒ…ëª¨ë¸ | ê°€ì¤‘ í‰ê·  ì•™ìƒë¸” |
    |--------------------------:|---------:|---------:|-------------:|-------------:|---------:|---------:|---------:|-----------:|-----------------:|
    | Mean Absolute Error (MAE) |   2.8446 |   2.1745 |       2.6267 |       2.0439 |   2.8482 |   3.3502 |   2.0510 |     1.9960 |           1.9760 |
    |  Mean Squared Error (MSE) |  13.6133 |  10.8285 |      11.5584 |       7.5495 |  13.6438 |  17.2110 |   7.6402 |     7.7054 |           7.1979 |
    |                  RÂ² Score |   0.8076 |   0.8469 |       0.8366 |       0.8933 |   0.8072 |   0.7568 |   0.8920 |     0.8911 |           0.8982 |

  - ìƒê°í•˜ê¸°ì—ëŠ” ë„ˆë¬´ ìƒê´€ê´€ê³„ê°€ ì¡°ê¸ˆ ìˆëŠ” ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ì„œ RÂ²ê°€ ë†’ì•„ì§„ê²Œ ì•„ë‹Œê°€ ìƒê°ì´ ë“ ë‹¤(ë¬¼ë¡  ë‚®ì•„ì§„ ë°ì´í„°ë„ ìˆìŒ)
  - MAEì™€ MSEê°€ ë‚®ì•„ì§€ì§€ ì•Šê³  ìƒìŠ¹í–ˆìœ¼ë¯€ë¡œ ì¢‹ì€ ëª¨ë¸ì´ë¼ê³  íŒë‹¨í•˜ê¸°ì—ëŠ” ì–´ë ¤ìš¸ ê²ƒ ê°™ë‹¤.